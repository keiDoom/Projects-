{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "00994a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from plotly import graph_objects as go\n",
    "\n",
    "from scapy.all import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "ee842ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('main').getOrCreate()\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "sparkContext = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "65941182",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder = \"C:\\\\Users\\\\igor.beryozkin\\\\Desktop\\\\docs\\\\PCAP\"\n",
    "\n",
    "files_list = []\n",
    "for file in os.listdir(main_folder):\n",
    "    pcap_folder = 'C:\\\\Users\\\\igor.beryozkin\\\\Desktop\\\\docs\\\\PCAP\\\\{0}'.format(file)\n",
    "    files_list.append(pcap_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "e13dd472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\igor.beryozkin\\\\Desktop\\\\docs\\\\PCAP\\\\1_pack.pcap',\n",
       " 'C:\\\\Users\\\\igor.beryozkin\\\\Desktop\\\\docs\\\\PCAP\\\\2_pack.pcap',\n",
       " 'C:\\\\Users\\\\igor.beryozkin\\\\Desktop\\\\docs\\\\PCAP\\\\3_pack.pcap',\n",
       " 'C:\\\\Users\\\\igor.beryozkin\\\\Desktop\\\\docs\\\\PCAP\\\\4_pack.pcap',\n",
       " 'C:\\\\Users\\\\igor.beryozkin\\\\Desktop\\\\docs\\\\PCAP\\\\5_pack.pcap',\n",
       " 'C:\\\\Users\\\\igor.beryozkin\\\\Desktop\\\\docs\\\\PCAP\\\\6_pack.pcap',\n",
       " 'C:\\\\Users\\\\igor.beryozkin\\\\Desktop\\\\docs\\\\PCAP\\\\7_pack.pcap',\n",
       " 'C:\\\\Users\\\\igor.beryozkin\\\\Desktop\\\\docs\\\\PCAP\\\\8_pack.pcap',\n",
       " 'C:\\\\Users\\\\igor.beryozkin\\\\Desktop\\\\docs\\\\PCAP\\\\9_pack.pcap']"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "76fbe748",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Таблица признаков и их описание\n",
    "F_columns = ['№', 'Признак', 'Описание']\n",
    "data = [('1','adsada','asdasdasdassd')]\n",
    "F_data = [(1, 'Размер запроса в байтах', 'Отклонение от средних показателей 40-60 байт в сторону увеличения может говорить о скрытом канале'),\n",
    "          (2, 'Необычный тип трафика', 'Если в трафике используются данные вида TXT, NULL, KEY - это говорит об аномалии'),\n",
    "          (3, 'Длина имени запроса', 'Имена используемые злоумышленниками могут достигать 200 символов - средняя 30 символов')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "ac23f36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feauture_DF = spark.createDataFrame(F_data, F_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "070e4a34",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+-------------------------------------------------------------------------------------------------+\n",
      "|№  |Признак                |Описание                                                                                         |\n",
      "+---+-----------------------+-------------------------------------------------------------------------------------------------+\n",
      "|1  |Размер запроса в байтах|Отклонение от средних показателей 40-60 байт в сторону увеличения может говорить о скрытом канале|\n",
      "|2  |Необычный тип трафика  |Если в трафике используются данные вида TXT, NULL, KEY - это говорит об аномалии                 |\n",
      "|3  |Длина имени запроса    |Имена используемые злоумышленниками могут достигать 200 символов - средняя 30 символов           |\n",
      "+---+-----------------------+-------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feauture_DF.show(truncate= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "e203d298",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DNS TUNNELING PCAP FILE FOR EXAMPLE \n",
    "\n",
    "#ioline = 'C://Users//igor.beryozkin//Downloads/dns-tunnel-iodine.pcap'\n",
    "#a = rdpcap(ioline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "18c3443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractData (file):\n",
    "    extracted_data = []\n",
    "    read_file = rdpcap(file)\n",
    "    \n",
    "    for pkt in read_file:\n",
    "\n",
    "        if pkt.haslayer(DNSQR):\n",
    "            query = pkt.getlayer(DNSQR)\n",
    "\n",
    "            qtype_field = query.get_field('qtype')\n",
    "            qtype_data = qtype_field.i2repr(query, query.qtype)\n",
    "\n",
    "            qname_field = query.get_field('qname')\n",
    "            qname_data = qname_field.i2repr(query, query.qname)\n",
    "            \n",
    "            extracted_data.append((qtype_data, qname_data.__len__(), pkt.len)) #TODO: rdd for optimizing processing\n",
    "            \n",
    "    return extracted_data #---> [(...), (...), ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "bc65eee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: recreate func, use rdd for create dataframe, instead of list's\n",
    "\n",
    "def CreateDataFrame (filesList, dfSchema):\n",
    "    main_DF = spark.createDataFrame([], dfSchema) \n",
    "    \n",
    "    for file in filesList:\n",
    "        rdd = sparkContext.parallelize(ExtractData(file))\n",
    "        \n",
    "    return main_DF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "355aac10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_schema = StructType([StructField('TrafficType', StringType(), True),\n",
    "                             StructField('NameLenght', IntegerType(), True),\n",
    "                             StructField('Bytes', IntegerType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "69e16b62",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [213]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m collected_data_df \u001b[38;5;241m=\u001b[39m \u001b[43mCreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiles_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_schema\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [211]\u001b[0m, in \u001b[0;36mCreateDataFrame\u001b[1;34m(filesList, dfSchema)\u001b[0m\n\u001b[0;32m      2\u001b[0m main_DF \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame([], dfSchema)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m filesList:\n\u001b[1;32m----> 5\u001b[0m     tmp_DF \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mExtractData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdfSchema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     main_DF \u001b[38;5;241m=\u001b[39m main_DF\u001b[38;5;241m.\u001b[39munion(tmp_DF)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m main_DF\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pyspark\\sql\\session.py:894\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    889\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pandas\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[0;32m    890\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m    891\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[0;32m    892\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[0;32m    893\u001b[0m     )\n\u001b[1;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    896\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pyspark\\sql\\session.py:936\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    934\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[0;32m    935\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 936\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromLocal(\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m, schema)\n\u001b[0;32m    937\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    938\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4de905",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collected_data_df = CreateDataFrame(files_list, df_schema)\n",
    "#writed_df_parquet = collected_data_df.write.parquet('C:\\\\Users\\\\igor.beryozkin\\\\Desktop\\\\docs\\\\PARQUET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bf570f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RDD-create example.\n",
    "\n",
    "# data = sparkContext.parallelize(ExtractData(files_list[1]))\n",
    "# dfdf = spark.createDataFrame(data, df_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faf5dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92206db9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
