{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7b7410d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T14:49:02.293493Z",
     "iopub.status.busy": "2022-10-05T14:49:02.292906Z",
     "iopub.status.idle": "2022-10-05T14:49:02.305132Z",
     "shell.execute_reply": "2022-10-05T14:49:02.299649Z",
     "shell.execute_reply.started": "2022-10-05T14:49:02.293370Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^.^\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    print (\"^.^\")\n",
    "else:\n",
    "    print (\">.<\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cd48b04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-08T11:43:23.673884Z",
     "iopub.status.busy": "2022-10-08T11:43:23.673364Z",
     "iopub.status.idle": "2022-10-08T11:43:25.136538Z",
     "shell.execute_reply": "2022-10-08T11:43:25.135473Z",
     "shell.execute_reply.started": "2022-10-08T11:43:23.673844Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import Row\n",
    "\n",
    "import random \n",
    "\n",
    "from matplotlib import pyplot as plta\n",
    "%matplotlib inline\n",
    "from plotly import graph_objects as go\n",
    "\n",
    "from scapy.all import *\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import psutil\n",
    "\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5511654",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-08T11:43:45.630815Z",
     "iopub.status.busy": "2022-10-08T11:43:45.630204Z",
     "iopub.status.idle": "2022-10-08T11:43:52.665273Z",
     "shell.execute_reply": "2022-10-08T11:43:52.664220Z",
     "shell.execute_reply.started": "2022-10-08T11:43:45.630767Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<SparkContext master=local[*] appName=MAIN>,\n",
       " <pyspark.sql.session.SparkSession at 0x7fa013587c70>,\n",
       " [('spark.executor.memory', '34g'),\n",
       "  ('spark.shuffle.service.enabled', 'False'),\n",
       "  ('spark.sql.repl.eagerEval.enabled', 'True'),\n",
       "  ('spark.driver.cores', '2'),\n",
       "  ('spark.executor.instance', '2'),\n",
       "  ('spark.app.submitTime', '1666032160740'),\n",
       "  ('spark.driver.memory', '16g'),\n",
       "  ('spark.rdd.compress', 'False'),\n",
       "  ('spark.executor.cores', '4'),\n",
       "  ('spark.dynamicAllocation.enabled', 'False'),\n",
       "  ('spark.app.name', 'MAIN'),\n",
       "  ('spark.driver.extraClassPath',\n",
       "   '/home/kei/.local/lib/python3.8/site-packages/sparkmonitor/listener_2.12.jar'),\n",
       "  ('spark.extraListeners',\n",
       "   'sparkmonitor.listener.JupyterSparkMonitorListener'),\n",
       "  ('spark.master', 'local[*]'),\n",
       "  ('spark.submit.pyFiles', ''),\n",
       "  ('spark.submit.deployMode', 'client'),\n",
       "  ('spark.ui.showConsoleProgress', 'true')])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (SparkSession.builder\n",
    "                     .config('spark.app.name', 'MAIN')\n",
    "                     .config('spark.driver.memory', '16g')\n",
    "                     .config('spark.driver.cores', '2')\n",
    "                     .config('spark.executor.cores', '4')\n",
    "                     .config('spark.executor.memory', '34g')\n",
    "                     .config('spark.executor.instance', '2')\n",
    "                     .config('spark.dynamicAllocation.enabled', False)\n",
    "                     .config('spark.shuffle.service.enabled', False)\n",
    "                     .config('spark.rdd.compress', False)\n",
    "                     .config('spark.sql.repl.eagerEval.enabled', True)\n",
    "                     .config('spark.extraListeners', 'sparkmonitor.listener.JupyterSparkMonitorListener')\n",
    "                     .config('spark.driver.extraClassPath', '/home/kei/.local/lib/python3.8/site-packages/sparkmonitor/listener_2.12.jar')\n",
    "                     #.config('spark.delight.accessToken.secret', 'f463ad09da4d1582f4508b7e3c894e90efc5777c00985965976801cc96a9b2b5')\n",
    "                     #.config('spark.delight.appNameOverride', 'spark.app.name')\n",
    "                     .getOrCreate())\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "sparkConf = SparkConf().getAll()\n",
    "\n",
    "\n",
    "sc, spark, sparkConf[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13e8484b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T14:49:10.163240Z",
     "iopub.status.busy": "2022-10-05T14:49:10.162690Z",
     "iopub.status.idle": "2022-10-05T14:49:27.457556Z",
     "shell.execute_reply": "2022-10-05T14:49:27.456228Z",
     "shell.execute_reply.started": "2022-10-05T14:49:10.163191Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2069\n",
      "2069\n"
     ]
    }
   ],
   "source": [
    "b=sc.broadcast([3,10]) #Creating a broadcast variable available on all executors\n",
    "a=sc.accumulator(0)   #Creating an accumulator for adding values across executors\n",
    "RDD0=sc.parallelize([y for y in range(0,5)]) #RDD from input python collection\n",
    "RDD2=sc.parallelize([z for z in range(1, 10)])\n",
    "RDD1=RDD0.cartesian(RDD2)\n",
    "cached=RDD2.cache() #Testing cached RDD\n",
    "RDD22=RDD1.map(lambda x:x[0]+x[1]+b.value[0])\n",
    "RDD3=RDD22.repartition(5) # To trigger a new stage.\n",
    "RDD4=RDD2.map(lambda x: 3*x-b.value[0])\n",
    "RDD5=RDD3.filter(lambda x:x%2==0)\n",
    "RDD6=RDD4.filter(lambda x:x%2!=0)\n",
    "RDD7=RDD5.cartesian(RDD6)\n",
    "RDD8=RDD7.flatMap(lambda x: [x[i] for i in range(0,2)])\n",
    "RDD9=RDD8.union(cached)\n",
    "ans=RDD9.reduce(lambda x,y: x+y) # Doing a simple sum on the random data.\n",
    "print(ans)\n",
    "def f(x):\n",
    "    global a\n",
    "    time.sleep(0.58) #Making the job run a little longer\n",
    "    a+=x\n",
    "RDD9.foreach(f)\n",
    "print(a.value)\n",
    "#Display should appear automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "678e719e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T14:49:27.463300Z",
     "iopub.status.busy": "2022-10-05T14:49:27.462793Z",
     "iopub.status.idle": "2022-10-05T14:49:27.471415Z",
     "shell.execute_reply": "2022-10-05T14:49:27.468243Z",
     "shell.execute_reply.started": "2022-10-05T14:49:27.463260Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mainFolder = \"/home/kei/Desktop/data/innostage/innostage_task_3/pcap_data/capture_packets/\"\n",
    "\n",
    "filesList = []\n",
    "for file in os.listdir(mainFolder):\n",
    "    pcapFolder = '/home/kei/Desktop/data/innostage/innostage_task_3/pcap_data/capture_packets/{0}'.format(file)\n",
    "    filesList.append(pcapFolder)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f7aa3e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T14:49:27.476842Z",
     "iopub.status.busy": "2022-10-05T14:49:27.476329Z",
     "iopub.status.idle": "2022-10-05T14:49:27.492602Z",
     "shell.execute_reply": "2022-10-05T14:49:27.491098Z",
     "shell.execute_reply.started": "2022-10-05T14:49:27.476798Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Таблица признаков и их описание\n",
    "F_columns = ['№', 'Признак', 'Описание']\n",
    "F_data = [(1, 'Размер запроса в байтах', 'Отклонение от средних показателей 40-60 байт в сторону увеличения может говорить о скрытом канале'),\n",
    "          (2, 'Необычный тип трафика', 'Если в трафике используются данные вида TXT, NULL, KEY - это говорит об аномалии'),\n",
    "          (3, 'Длина имени запроса', 'Имена используемые злоумышленниками могут достигать 200 символов - средняя 30 символов')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eeac5e93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T14:49:27.499220Z",
     "iopub.status.busy": "2022-10-05T14:49:27.498283Z",
     "iopub.status.idle": "2022-10-05T14:49:32.097663Z",
     "shell.execute_reply": "2022-10-05T14:49:32.096317Z",
     "shell.execute_reply.started": "2022-10-05T14:49:27.499159Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "feauture_DF = spark.createDataFrame(F_data, F_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "387d79e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T14:49:32.099783Z",
     "iopub.status.busy": "2022-10-05T14:49:32.099262Z",
     "iopub.status.idle": "2022-10-05T14:49:34.961025Z",
     "shell.execute_reply": "2022-10-05T14:49:34.959718Z",
     "shell.execute_reply.started": "2022-10-05T14:49:32.099733Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>№</th><th>Признак</th><th>Описание</th></tr>\n",
       "<tr><td>1</td><td>Размер запроса в ...</td><td>Отклонение от сре...</td></tr>\n",
       "<tr><td>2</td><td>Необычный тип тра...</td><td>Если в трафике ис...</td></tr>\n",
       "<tr><td>3</td><td>Длина имени запроса</td><td>Имена используемы...</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+--------------------+--------------------+\n",
       "|  №|             Признак|            Описание|\n",
       "+---+--------------------+--------------------+\n",
       "|  1|Размер запроса в ...|Отклонение от сре...|\n",
       "|  2|Необычный тип тра...|Если в трафике ис...|\n",
       "|  3| Длина имени запроса|Имена используемы...|\n",
       "+---+--------------------+--------------------+"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feauture_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed4248fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T14:49:34.968173Z",
     "iopub.status.busy": "2022-10-05T14:49:34.964294Z",
     "iopub.status.idle": "2022-10-05T14:49:34.989620Z",
     "shell.execute_reply": "2022-10-05T14:49:34.988250Z",
     "shell.execute_reply.started": "2022-10-05T14:49:34.968091Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_data(files):\n",
    "    data = []\n",
    "\n",
    "    for file in files:\n",
    "        \n",
    "        for pkt in PcapReader(file):\n",
    "            \n",
    "            if pkt.haslayer(DNSQR):\n",
    "                query = pkt.getlayer(DNSQR)\n",
    "                \n",
    "                qtype_field = query.get_field('qtype')\n",
    "                qtype_data = qtype_field.i2repr(query, query.qtype)\n",
    "                \n",
    "                qname_field = query.get_field('qname')\n",
    "                qname_data = qname_field.i2repr(query, query.qname)\n",
    "                \n",
    "                data.append((qtype_data, qname_data.__len__(), pkt.len))\n",
    "                \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04af223c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T14:49:34.996865Z",
     "iopub.status.busy": "2022-10-05T14:49:34.994742Z",
     "iopub.status.idle": "2022-10-05T14:49:35.017050Z",
     "shell.execute_reply": "2022-10-05T14:49:35.015806Z",
     "shell.execute_reply.started": "2022-10-05T14:49:34.996783Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_run_time(startTime):\n",
    "    return time.time() - startTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa764ad0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T14:49:35.020244Z",
     "iopub.status.busy": "2022-10-05T14:49:35.019122Z",
     "iopub.status.idle": "2022-10-05T14:49:35.937117Z",
     "shell.execute_reply": "2022-10-05T14:49:35.935286Z",
     "shell.execute_reply.started": "2022-10-05T14:49:35.020175Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('TrafficType', StringType(), True), StructField('NameLenght', IntegerType(), True), StructField('Bytes', IntegerType(), True)])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfSchema = StructType([StructField('TrafficType', StringType(), True),\n",
    "                             StructField('NameLenght', IntegerType(), True),\n",
    "                             StructField('Bytes', IntegerType(), True)])\n",
    "dfSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7b84178c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T14:49:35.940362Z",
     "iopub.status.busy": "2022-10-05T14:49:35.939523Z",
     "iopub.status.idle": "2022-10-05T14:49:35.945949Z",
     "shell.execute_reply": "2022-10-05T14:49:35.944693Z",
     "shell.execute_reply.started": "2022-10-05T14:49:35.940286Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#data = extract_data(filesList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a44655e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T14:49:35.948393Z",
     "iopub.status.busy": "2022-10-05T14:49:35.947873Z",
     "iopub.status.idle": "2022-10-05T14:49:35.956316Z",
     "shell.execute_reply": "2022-10-05T14:49:35.955022Z",
     "shell.execute_reply.started": "2022-10-05T14:49:35.948348Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#with open('data.txt', 'w') as file_write:\n",
    "#    json.dump(data, file_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2c3e513d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T14:49:35.958496Z",
     "iopub.status.busy": "2022-10-05T14:49:35.957972Z",
     "iopub.status.idle": "2022-10-05T14:49:36.134739Z",
     "shell.execute_reply": "2022-10-05T14:49:36.133444Z",
     "shell.execute_reply.started": "2022-10-05T14:49:35.958448Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('/home/kei/Desktop/data/innostage/innostage_task_3/extracted_data/data.txt', 'r') as file_read:\n",
    "    data = json.load(file_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a7ed314b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T14:49:36.136711Z",
     "iopub.status.busy": "2022-10-05T14:49:36.136211Z",
     "iopub.status.idle": "2022-10-05T14:49:36.266672Z",
     "shell.execute_reply": "2022-10-05T14:49:36.265067Z",
     "shell.execute_reply.started": "2022-10-05T14:49:36.136663Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "main_DF = spark.createDataFrame(sc.parallelize(data), dfSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7a842beb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T14:49:36.272210Z",
     "iopub.status.busy": "2022-10-05T14:49:36.271040Z",
     "iopub.status.idle": "2022-10-05T14:49:36.593416Z",
     "shell.execute_reply": "2022-10-05T14:49:36.592178Z",
     "shell.execute_reply.started": "2022-10-05T14:49:36.272144Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----+\n",
      "|TrafficType|NameLenght|Bytes|\n",
      "+-----------+----------+-----+\n",
      "|          A|        24|   67|\n",
      "|       AAAA|        24|   67|\n",
      "|          A|        18|   61|\n",
      "|       AAAA|        18|   61|\n",
      "|       AAAA|        22|   65|\n",
      "|          A|        22|   65|\n",
      "|       AAAA|        22|   65|\n",
      "|          A|        22|   65|\n",
      "|       AAAA|        22|  122|\n",
      "|       AAAA|        22|  122|\n",
      "|          A|        22|   81|\n",
      "|          A|        22|   81|\n",
      "|          A|        24|   67|\n",
      "|       AAAA|        24|   67|\n",
      "|        PTR|        27|   70|\n",
      "|        PTR|        27|  105|\n",
      "|          A|        18|   61|\n",
      "|       AAAA|        18|   61|\n",
      "|          A|        15|   58|\n",
      "|          A|        15|  223|\n",
      "+-----------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main_DF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "18f6cec0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T14:49:48.658625Z",
     "iopub.status.busy": "2022-10-05T14:49:48.658084Z",
     "iopub.status.idle": "2022-10-05T14:49:48.662829Z",
     "shell.execute_reply": "2022-10-05T14:49:48.661559Z",
     "shell.execute_reply.started": "2022-10-05T14:49:48.658585Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#main_DF.write.parquet('/home/kei/Desktop/data/docs/PARQUET/traffic_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2a61cc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T14:49:49.459085Z",
     "iopub.status.busy": "2022-10-05T14:49:49.458559Z",
     "iopub.status.idle": "2022-10-05T14:49:50.709419Z",
     "shell.execute_reply": "2022-10-05T14:49:50.707953Z",
     "shell.execute_reply.started": "2022-10-05T14:49:49.459046Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fileParquet = spark.read.parquet('/home/kei/Desktop/data/innostage/innostage_task_3/parquet_data/traffic_data_rdy/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b9118f",
   "metadata": {},
   "source": [
    "______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98000e1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# #TODO \n",
    "    - Подогнать набор данных так, чтобы результаты обработки максимально соответствовали конфигурации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e9e4b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2989\n"
     ]
    }
   ],
   "source": [
    "#Processing Data Check.\n",
    "#DATA_PROCESSING_RESULT = []\n",
    "data_for_shit = []\n",
    "\n",
    "\n",
    "parquetPath = '/home/kei/Desktop/data/innostage/innostage_task_3/parquet_data/'\n",
    "\n",
    "field = parquetPath + 'traffic_data_field1'\n",
    "\n",
    "with open('/home/kei/Desktop/data/innostage/innostage_task_3/extracted_data/data.txt', 'r') as file_read:\n",
    "    data = json.load(file_read)\n",
    "\n",
    "\n",
    "for i in range(1, 2):\n",
    "    \n",
    "    b=sc.broadcast([3,10]) #Creating a broadcast variable available on all executors\n",
    "    a=sc.accumulator(0)   #Creating an accumulator for adding values across executors\n",
    "    RDD0=sc.parallelize([y for y in range(0,7)]) #RDD from input python collection\n",
    "    RDD2=sc.parallelize([z for z in range(1, 10)])\n",
    "    RDD1=RDD0.cartesian(RDD2)\n",
    "    cached=RDD2.cache() #Testing cached RDD\n",
    "    RDD22=RDD1.map(lambda x:x[0]+x[1]+b.value[0])\n",
    "    RDD3=RDD22.repartition(5) # To trigger a new stage.\n",
    "    RDD4=RDD2.map(lambda x: 3*x-b.value[0])\n",
    "    RDD5=RDD3.filter(lambda x:x%2==0)\n",
    "    RDD6=RDD4.filter(lambda x:x%2!=0)\n",
    "    RDD7=RDD5.cartesian(RDD6)\n",
    "    RDD8=RDD7.flatMap(lambda x: [x[i] for i in range(0,2)])\n",
    "    RDD9=RDD8.union(cached)\n",
    "    ans=RDD9.reduce(lambda x,y: x+y) # Doing a simple sum on the random data.\n",
    "    print(ans)\n",
    "    def f(x):\n",
    "        global a\n",
    "        time.sleep(0.085) #Making the job run a little longer\n",
    "        a+=x\n",
    "    RDD9.foreach(f)\n",
    "\n",
    "\n",
    "\n",
    "main_DF = spark.createDataFrame(sc.parallelize(data_for_shit), dfSchema)\n",
    "#main_DF.write.parquet(field)\n",
    "\n",
    "\n",
    "fileParquet = spark.read.parquet(field)\n",
    "#---------------------------------------------------------------------------\n",
    "\n",
    "#with open('process_result_field1.txt', 'w') as file_write:\n",
    "#    json.dump(DATA_PROCESSING_RESULT, file_write)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec915f7",
   "metadata": {},
   "source": [
    "______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-______-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "93afef25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-08T11:43:59.524068Z",
     "iopub.status.busy": "2022-10-08T11:43:59.523545Z",
     "iopub.status.idle": "2022-10-08T11:43:59.531852Z",
     "shell.execute_reply": "2022-10-08T11:43:59.530451Z",
     "shell.execute_reply.started": "2022-10-08T11:43:59.524025Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "processingTestSchema = ['Filed', 'Driver Memory', 'Executor Memory', 'Executor Instance', 'Driver Cores', 'Processing Time', 'Bytes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b725c1d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-05T15:26:34.525596Z",
     "iopub.status.busy": "2022-10-05T15:26:34.525041Z",
     "iopub.status.idle": "2022-10-05T15:26:34.532005Z",
     "shell.execute_reply": "2022-10-05T15:26:34.530750Z",
     "shell.execute_reply.started": "2022-10-05T15:26:34.525554Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#json_data = []\n",
    "#for eachFile in os.listdir('/home/kei/Desktop/data/docs/JSON/'):\n",
    "#    \n",
    "#    with open(str(eachFile), 'r') as file_read:\n",
    "#        json_data.append(json.load(file_read))\n",
    "#\n",
    "#        \n",
    "#jsonAggregatedData = []\n",
    "#for i in json_data:\n",
    "#    for k in i:\n",
    "#        jsonAggregatedData.append(tuple(k))\n",
    "\n",
    "\n",
    "#with open('json_data.txt', 'w') as file:\n",
    "#    json.dump(json_data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a78c08ec",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-10-08T11:48:30.799979Z",
     "iopub.status.busy": "2022-10-08T11:48:30.799425Z",
     "iopub.status.idle": "2022-10-08T11:48:30.836967Z",
     "shell.execute_reply": "2022-10-08T11:48:30.835056Z",
     "shell.execute_reply.started": "2022-10-08T11:48:30.799931Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-a257762ae7ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_DF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessingTestSchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    892\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             )\n\u001b[0;32m--> 894\u001b[0;31m         return self._create_dataframe(\n\u001b[0m\u001b[1;32m    895\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    934\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    634\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m                     \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m                     \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "test_DF = spark.createDataFrame(data, processingTestSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b1b3e22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing Data Check.\n",
    "#DATA_PROCESSING_RESULT = []\n",
    "#parquetPath = 'C:\\\\Users\\\\igor.beryozkin\\\\Desktop\\\\docs\\\\PARQUET\\\\'\n",
    "#\n",
    "#--------------------------------------------------------------------------\n",
    "#\n",
    "#field: 1.\n",
    "#startTime = time.time()\n",
    "#\n",
    "#field_1 = parquetPath + 'traffic_data_field1\\\\'\n",
    "#\n",
    "#with open('C:/Users/igor.beryozkin/Desktop/docs/TXT/innostage_task3/extracted_data/data.txt', 'r') as file_read:\n",
    "#   data = json.load(file_read)\n",
    "##\n",
    "#\n",
    "#main_DF = spark.createDataFrame(sparkContext.parallelize(data), dfSchema)\n",
    "#    \n",
    "#main_DF.write.parquet(field_1)\n",
    "#fileParquet = spark.read.parquet(field_1)\n",
    "#\n",
    "#DATA_PROCESSING_RESULT.append(('field 1', SparkConf().get('spark.driver.memory'),\n",
    "#                                SparkConf().get('spark.executor.memory'),\n",
    "#                                SparkConf().get('spark.executor.instance'),\n",
    "#                                SparkConf().get('spark.driver.cores'),\n",
    "#                                get_run_time(startTime),\n",
    "#                                get_path_size_Bytes(field_1)))\n",
    "#\n",
    "#---------------------------------------------------------------------------\n",
    "#\n",
    "#with open('process_result_field1.txt', 'w') as file_write:\n",
    "#    json.dump(DATA_PROCESSING_RESULT, file_write)\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
